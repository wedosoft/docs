---
sidebar_position: 3
---

# 문제 조사 및 진단

문제 조사 및 진단은 식별된 문제의 근본 원인을 체계적으로 파악하기 위한 깊이 있는 분석과 검증 과정입니다.

:::info 문제 조사 전 준비사항
- **조사팀 구성**: 기술 전문가, 비즈니스 담당자, 외부 전문가 등 균형잡힌 팀 구성
- **조사 도구 준비**: 로그 분석 도구, 성능 모니터링 시스템, 네트워크 분석 도구
- **조사 계획 수립**: 체계적인 조사 방법론과 타임라인 설정
:::

## 체계적 조사 프로세스

### 1단계: 조사 준비 및 계획

**조사팀 구성**
```markdown
👥 핵심 조사팀:
- 문제 관리자 (Problem Manager) - 조사 총괄
- 기술 전문가 (Subject Matter Expert) - 해당 기술 영역 전문가
- 시스템 관리자 (System Administrator) - 인프라 및 운영 전문가
- 비즈니스 담당자 (Business Representative) - 업무 영향도 분석

🔍 확장 조사팀 (필요시):
- 외부 벤더 기술진
- 보안 전문가
- 네트워크 엔지니어
- 데이터베이스 관리자
```

**조사 계획 수립**
```markdown
📋 조사 계획 템플릿:
1. 조사 목표 및 범위 정의
2. 조사 방법론 선정
3. 필요 자원 및 도구 확보
4. 조사 일정 및 마일스톤
5. 위험 요소 및 제약사항
6. 의사소통 계획
```

### 2단계: 데이터 수집 및 분석

**시스템 로그 분석**

| 로그 유형 | 수집 기간 | 분석 포인트 | 도구 |
|-----------|-----------|-------------|------|
| **시스템 로그** | 문제 발생 전후 24시간 | 에러 메시지, 경고, 성능 이상 | Splunk, ELK Stack |
| **애플리케이션 로그** | 문제 발생 전후 48시간 | 트랜잭션 오류, 응답 시간 | APM 도구 |
| **데이터베이스 로그** | 문제 발생 전후 72시간 | 슬로우 쿼리, 락 경합, 데드락 | DB 모니터링 도구 |
| **네트워크 로그** | 문제 발생 전후 24시간 | 패킷 손실, 지연, 대역폭 | 네트워크 분석기 |

**성능 메트릭 분석**
```markdown
📊 핵심 성능 지표:
시스템 리소스:
- CPU 사용률 패턴
- 메모리 사용량 추이
- 디스크 I/O 성능
- 네트워크 대역폭 활용도

애플리케이션 성능:
- 응답 시간 분포
- 처리량 (TPS) 변화
- 에러율 추이
- 사용자 세션 패턴
```

### 3단계: 가설 수립 및 검증

**가설 생성 방법론**

**브레인스토밍 세션**
```markdown
🧠 효과적인 브레인스토밍:
1. 모든 가능한 원인 나열 (판단 보류)
2. 유사한 원인들을 그룹화
3. 데이터 기반 가능성 평가
4. 검증 가능한 가설로 구체화

💡 가설 평가 기준:
- 수집된 데이터와의 일치성
- 기술적 타당성
- 발생 가능성
- 검증 용이성
```

**가설 우선순위 결정**
```markdown
우선순위 = (가능성 × 3) + (영향도 × 2) + (검증용이성 × 1)

가능성 점수 (1-5):
5: 데이터가 강하게 지지
4: 데이터가 부분적으로 지지
3: 데이터와 중립적
2: 데이터가 부분적으로 반박
1: 데이터가 강하게 반박

영향도 점수 (1-5):
5: 전체 문제 완전 설명
4: 주요 증상 대부분 설명
3: 일부 증상 설명
2: 부분적 연관성
1: 미미한 연관성
```

## 진단 도구 및 기법

### 로그 분석 도구

**통합 로그 관리**
```markdown
🔧 Splunk 활용 예시:
search index=application error
| stats count by source, error_type
| sort -count
| eval percentage=round(count/total*100,2)

🔧 ELK Stack 쿼리:
GET /logs/_search
{
  "query": {
    "bool": {
      "must": [
        {"range": {"@timestamp": {"gte": "now-24h"}}},
        {"match": {"level": "ERROR"}}
      ]
    }
  },
  "aggs": {
    "error_patterns": {
      "terms": {"field": "message.keyword"}
    }
  }
}
```

**실시간 모니터링**
- Grafana 대시보드를 통한 실시간 메트릭 시각화
- Prometheus 메트릭 수집 및 알림
- New Relic APM을 통한 애플리케이션 성능 추적

### 네트워크 분석 도구

**패킷 분석**
```markdown
🌐 Wireshark 분석 체크리스트:
- [ ] TCP 재전송 패킷 확인
- [ ] 응답 시간 측정
- [ ] 패킷 손실률 계산
- [ ] 대역폭 사용 패턴 분석
- [ ] 프로토콜별 트래픽 분포

🔍 네트워크 성능 지표:
- RTT (Round Trip Time)
- 패킷 손실률
- 지터 (Jitter)
- 대역폭 활용률
```

### 시스템 성능 분석

**리소스 사용 패턴 분석**
```bash
# CPU 사용률 상세 분석
top -H -p <PID>
iostat -x 1 10
vmstat 1 10

# 메모리 사용 패턴 분석
free -h
cat /proc/meminfo
pmap -x <PID>

# 디스크 I/O 분석
iotop -o
lsof | grep deleted
```

## 실무 활용 예시

### 상황 1: 웹 애플리케이션 간헐적 응답 지연
**목표**: 불규칙하게 발생하는 웹 서비스 응답 지연의 근본 원인 파악

**조사 과정:**

1. **초기 데이터 수집**
   ```markdown
   📊 수집된 증상:
   - 응답 지연: 평상시 2초 → 지연 시 15-30초
   - 발생 빈도: 하루 3-5회, 불규칙
   - 영향 범위: 전체 사용자
   - 지속 시간: 2-5분 후 자동 복구
   ```

2. **가설 수립**
   ```markdown
   🔍 주요 가설:
   가설 1: 데이터베이스 락 경합 (가능성: 높음)
   가설 2: 가비지 컬렉션 지연 (가능성: 중간)
   가설 3: 네트워크 병목 현상 (가능성: 낮음)
   가설 4: 외부 API 호출 지연 (가능성: 중간)
   ```

3. **체계적 검증**
   
   **가설 1 검증: 데이터베이스 락 경합**
   ```sql
   -- 락 대기 상황 확인
   SELECT 
     waiting.pid AS waiting_pid,
     waiting.query AS waiting_query,
     blocking.pid AS blocking_pid,
     blocking.query AS blocking_query
   FROM pg_stat_activity waiting
   JOIN pg_stat_activity blocking 
   ON blocking.pid = ANY(pg_blocking_pids(waiting.pid));
   
   결과: 지연 시점과 락 대기 시점 완벽 일치 확인
   ```

   **가설 2 검증: 가비지 컬렉션**
   ```bash
   # GC 로그 분석
   grep "Full GC" gc.log | tail -20
   
   결과: Full GC 발생과 지연 시점 불일치
   ```

4. **근본 원인 확정**
   - 특정 배치 작업이 대용량 테이블에 배타적 락 설정
   - 웹 애플리케이션의 트랜잭션이 락 대기 상태 진입
   - 배치 작업 완료 후 정상 서비스 복구

**해결 방안**: 배치 작업 스케줄 조정 및 락 타임아웃 설정

:::success 조사 성공 요인
- 체계적인 가설 수립과 우선순위 설정
- 정량적 데이터 기반 객관적 검증
- 다각도 분석을 통한 정확한 원인 파악
:::

### 상황 2: 이메일 서비스 완전 중단
**목표**: 전사 이메일 서비스 중단의 신속한 원인 진단

**긴급 진단 프로세스:**

1. **즉시 대응팀 구성** (15분 내)
   ```markdown
   🚨 긴급 진단팀:
   - 인시던트 매니저 (총괄)
   - 이메일 시스템 전문가
   - 네트워크 엔지니어
   - 데이터베이스 관리자
   ```

2. **신속 진단 체크리스트**
   ```markdown
   ⚡ 1차 진단 (5분):
   - [ ] 서버 상태 확인 (ping, 서비스 status)
   - [ ] 네트워크 연결성 확인
   - [ ] 디스크 용량 확인
   - [ ] 프로세스 상태 확인

   ⚡ 2차 진단 (15분):
   - [ ] 시스템 로그 최신 에러 확인
   - [ ] 리소스 사용률 확인
   - [ ] 외부 의존성 확인
   - [ ] 최근 변경사항 검토
   ```

3. **원인 발견**
   ```bash
   # 디스크 용량 확인
   df -h
   /dev/sda1  100%  /var/log  # 로그 디스크 100% 사용
   
   # 대용량 파일 확인
   du -sh /var/log/* | sort -hr
   15G  /var/log/mail.log  # 비정상적 대용량 로그 파일
   ```

4. **즉시 복구 조치**
   - 대용량 로그 파일 아카이브 후 삭제
   - 이메일 서비스 재시작
   - 로그 로테이션 정책 재설정

**결과**: 30분 내 서비스 복구, 향후 재발 방지책 적용

### 상황 3: 복합 시스템 성능 저하
**목표**: 여러 시스템이 연관된 복잡한 성능 문제 진단

**종합 진단 접근법:**

1. **시스템 맵 기반 분석**
   ```markdown
   🗺️ 시스템 의존성 맵:
   사용자 → 로드밸런서 → 웹서버 → API서버 → 데이터베이스
              ↓            ↓         ↓
            캐시서버    큐시스템   외부API
   ```

2. **각 구간별 성능 측정**
   ```markdown
   📏 구간별 응답시간 (목표 vs 실제):
   - 로드밸런서: 50ms vs 45ms ✅
   - 웹서버: 200ms vs 180ms ✅
   - API서버: 500ms vs 2,000ms ❌
   - 데이터베이스: 100ms vs 150ms ⚠️
   - 외부API: 300ms vs 280ms ✅
   ```

3. **병목 구간 집중 분석**
   - API서버에서 주요 지연 발생 확인
   - CPU 사용률 정상, 메모리 사용률 높음
   - 메모리 누수 패턴 발견
   - 특정 모듈의 메모리 할당 문제 식별

**결과**: 정확한 병목 지점 파악으로 효율적 문제 해결

## 진단 품질 향상 방안

### 진단 표준화

**진단 체크리스트**
```markdown
📋 표준 진단 절차:
1단계: 기본 상태 확인 (5분)
- [ ] 서비스 가용성
- [ ] 시스템 리소스
- [ ] 네트워크 연결성

2단계: 로그 분석 (30분)
- [ ] 에러 로그 검토
- [ ] 성능 로그 분석
- [ ] 보안 로그 확인

3단계: 심화 분석 (2시간)
- [ ] 가설 기반 검증
- [ ] 성능 프로파일링
- [ ] 의존성 분석
```

**진단 문서화 템플릿**
```markdown
📄 진단 보고서 템플릿:
1. 문제 요약
2. 조사 방법론
3. 수집된 데이터
4. 분석 결과
5. 검증된 가설
6. 근본 원인
7. 추천 해결책
8. 후속 조치 계획
```

:::tip 효과적인 문제 진단을 위한 핵심 원칙
- **체계적 접근**: 감정이 아닌 데이터와 사실에 기반한 진단
- **가설 중심**: 명확한 가설 수립과 체계적 검증
- **협력적 진단**: 다분야 전문가들의 통합적 접근
- **지속적 학습**: 진단 과정과 결과의 문서화를 통한 역량 향상
:::

체계적인 문제 조사 및 진단을 통해 조직은 정확한 근본 원인을 파악하고, 효과적인 해결책을 개발할 수 있습니다.