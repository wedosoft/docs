# Analytics ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ë° ETL í”„ë¡œì„¸ìŠ¤

## ê°œìš”

Freshservice Analyticsì˜ ê°•ë ¥í•œ ë¶„ì„ ê¸°ëŠ¥ì€ ì²´ê³„ì ì¸ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì•„í‚¤í…ì²˜ì™€ íš¨ìœ¨ì ì¸ ETL(Extract, Transform, Load) í”„ë¡œì„¸ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. ì´ ë¬¸ì„œëŠ” Analytics í”Œë«í¼ì˜ ë°ì´í„° ì²˜ë¦¬ ë°©ì‹ê³¼ ìµœì í™” ë°©ë²•ì„ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

## ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì•„í‚¤í…ì²˜

### 1. ê³„ì¸µë³„ ë°ì´í„° êµ¬ì¡°

#### ìš´ì˜ ë°ì´í„° ê³„ì¸µ (Operational Layer)
```
ì‹¤ì‹œê°„ ìš´ì˜ ë°ì´í„°:
ğŸ“Š Tickets: ì‹¤ì‹œê°„ í‹°ì¼“ ì •ë³´
ğŸ‘¥ Users: ì‚¬ìš©ì ë° ìƒë‹´ì› ë°ì´í„°
ğŸ¢ Organizations: ì¡°ì§ ë° ë¶€ì„œ ì •ë³´
ğŸ’¼ Assets: IT ìì‚° ê´€ë¦¬ ë°ì´í„°
ğŸ”„ Changes: ë³€ê²½ ê´€ë¦¬ ê¸°ë¡
ğŸ“‹ Problems: ë¬¸ì œ ê´€ë¦¬ ë°ì´í„°
```

#### ìŠ¤í…Œì´ì§• ê³„ì¸µ (Staging Layer)
```
ë°ì´í„° ë³€í™˜ ì¤‘ê°„ ë‹¨ê³„:
- ì›ë³¸ ë°ì´í„° ì„ì‹œ ì €ì¥
- ë°ì´í„° í’ˆì§ˆ ê²€ì¦
- í˜•ì‹ í‘œì¤€í™”
- ì¤‘ë³µ ì œê±°
- ë°ì´í„° ì •í•©ì„± ê²€ì‚¬
```

#### ë°ì´í„° ë§ˆíŠ¸ ê³„ì¸µ (Data Mart Layer)
```
ë¶„ì„ìš© ìµœì í™” ë°ì´í„°:
ğŸ¯ Service Desk Mart: ì„œë¹„ìŠ¤ ë°ìŠ¤í¬ KPI
ğŸ“ˆ Performance Mart: ì„±ëŠ¥ ë¶„ì„ ë°ì´í„°
ğŸ‘¤ Customer Mart: ê³ ê° ì¤‘ì‹¬ ë¶„ì„
ğŸ”§ Asset Mart: ìì‚° ê´€ë¦¬ ë¶„ì„
ğŸ“Š Financial Mart: ë¹„ìš© ë° ROI ë¶„ì„
```

### 2. ë°ì´í„° ëª¨ë¸ë§

#### ì°¨ì› ëª¨ë¸ (Dimensional Model)
```
íŒ©íŠ¸ í…Œì´ë¸” (Fact Tables):
- FactTickets: í‹°ì¼“ ê´€ë ¨ ì¸¡ì •ê°’
- FactAssets: ìì‚° ê´€ë ¨ ë©”íŠ¸ë¦­
- FactSurveys: ë§Œì¡±ë„ ì¡°ì‚¬ ê²°ê³¼
- FactTimeTracking: ì‹œê°„ ì¶”ì  ë°ì´í„°

ì°¨ì› í…Œì´ë¸” (Dimension Tables):
- DimTime: ì‹œê°„ ì°¨ì› (ì¼, ì£¼, ì›”, ë¶„ê¸°, ë…„)
- DimAgent: ìƒë‹´ì› ì°¨ì›
- DimCustomer: ê³ ê° ì°¨ì›
- DimCategory: ì¹´í…Œê³ ë¦¬ ì°¨ì›
- DimPriority: ìš°ì„ ìˆœìœ„ ì°¨ì›
```

## ETL í”„ë¡œì„¸ìŠ¤ ìƒì„¸

### 1. ì¶”ì¶œ (Extract) ë‹¨ê³„

#### ì‹¤ì‹œê°„ ë°ì´í„° ìº¡ì²˜
```sql
-- ë³€ê²½ëœ í‹°ì¼“ ë°ì´í„° ì¶”ì¶œ
SELECT 
    id,
    subject,
    description,
    status,
    priority,
    created_at,
    updated_at,
    assigned_to,
    requester_id
FROM tickets 
WHERE updated_at > :last_extract_time
```

#### ì¦ë¶„ ì¶”ì¶œ ì „ëµ
```python
def extract_incremental_data(table_name, last_sync_time):
    """
    ì¦ë¶„ ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜
    """
    query = f"""
    SELECT * FROM {table_name} 
    WHERE modified_date > '{last_sync_time}'
    OR created_date > '{last_sync_time}'
    """
    
    return execute_query(query)
```

### 2. ë³€í™˜ (Transform) ë‹¨ê³„

#### ë°ì´í„° ì •ì œ ë° í‘œì¤€í™”
```python
def clean_ticket_data(raw_data):
    """
    í‹°ì¼“ ë°ì´í„° ì •ì œ í•¨ìˆ˜
    """
    cleaned_data = []
    
    for record in raw_data:
        # NULL ê°’ ì²˜ë¦¬
        record['priority'] = record['priority'] or 'Medium'
        record['category'] = record['category'] or 'General'
        
        # ë°ì´í„° í˜•ì‹ í‘œì¤€í™”
        record['created_at'] = standardize_datetime(record['created_at'])
        
        # ë°ì´í„° ê²€ì¦
        if validate_record(record):
            cleaned_data.append(record)
    
    return cleaned_data
```

#### ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì ìš©
```python
def apply_business_rules(ticket_data):
    """
    ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì ìš©
    """
    # SLA ê³„ì‚°
    ticket_data['sla_breached'] = calculate_sla_breach(
        ticket_data['created_at'],
        ticket_data['resolved_at'],
        ticket_data['priority']
    )
    
    # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
    ticket_data['first_response_time'] = calculate_first_response_time(
        ticket_data['created_at'],
        ticket_data['first_response_at']
    )
    
    # ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¥˜
    ticket_data['customer_segment'] = classify_customer_segment(
        ticket_data['requester_id']
    )
    
    return ticket_data
```

#### ì§‘ê³„ ë° ìš”ì•½ í…Œì´ë¸” ìƒì„±
```sql
-- ì¼ë³„ í‹°ì¼“ ìš”ì•½ í…Œì´ë¸”
CREATE TABLE daily_ticket_summary AS
SELECT 
    DATE(created_at) as ticket_date,
    priority,
    category,
    COUNT(*) as ticket_count,
    AVG(CASE WHEN resolved_at IS NOT NULL 
        THEN TIMESTAMPDIFF(HOUR, created_at, resolved_at) 
        END) as avg_resolution_time,
    COUNT(CASE WHEN sla_breached = 1 THEN 1 END) as sla_breach_count
FROM processed_tickets
GROUP BY DATE(created_at), priority, category;
```

### 3. ì ì¬ (Load) ë‹¨ê³„

#### ë°°ì¹˜ ë¡œë”© ì „ëµ
```python
def load_data_batch(data, table_name, batch_size=1000):
    """
    ë°°ì¹˜ ë‹¨ìœ„ ë°ì´í„° ë¡œë”©
    """
    for i in range(0, len(data), batch_size):
        batch = data[i:i + batch_size]
        
        try:
            insert_batch(table_name, batch)
            log_success(f"Loaded batch {i//batch_size + 1}")
            
        except Exception as e:
            log_error(f"Failed to load batch {i//batch_size + 1}: {e}")
            # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ë³„ë„ ì²˜ë¦¬
            handle_failed_batch(batch, table_name)
```

#### ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë¡œë“œ
```python
def stream_real_time_data():
    """
    ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
    """
    kafka_consumer = create_kafka_consumer('freshservice-events')
    
    for message in kafka_consumer:
        event_data = json.loads(message.value)
        
        # ì‹¤ì‹œê°„ ë³€í™˜
        transformed_data = transform_real_time_event(event_data)
        
        # ì¦‰ì‹œ ë¡œë“œ
        load_to_analytics_db(transformed_data)
        
        # ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸
        update_real_time_metrics(transformed_data)
```

## ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬

### 1. ë°ì´í„° ê²€ì¦ ê·œì¹™

#### í•„ìˆ˜ í•„ë“œ ê²€ì¦
```python
def validate_required_fields(record, required_fields):
    """
    í•„ìˆ˜ í•„ë“œ ê²€ì¦
    """
    for field in required_fields:
        if not record.get(field):
            raise ValidationError(f"Required field {field} is missing")
```

#### ë°ì´í„° íƒ€ì… ê²€ì¦
```python
def validate_data_types(record, field_types):
    """
    ë°ì´í„° íƒ€ì… ê²€ì¦
    """
    for field, expected_type in field_types.items():
        if field in record:
            if not isinstance(record[field], expected_type):
                record[field] = convert_type(record[field], expected_type)
```

#### ì°¸ì¡° ë¬´ê²°ì„± ê²€ì¦
```sql
-- ì™¸ë˜ í‚¤ ì œì•½ ì¡°ê±´ ê²€ì‚¬
SELECT t.id, t.assigned_to
FROM tickets t
LEFT JOIN agents a ON t.assigned_to = a.id
WHERE t.assigned_to IS NOT NULL 
AND a.id IS NULL;
```

### 2. ë°ì´í„° ì´ìƒ íƒì§€

#### í†µê³„ì  ì´ìƒ íƒì§€
```python
def detect_statistical_anomalies(data, column):
    """
    í†µê³„ì  ë°©ë²•ì„ ì‚¬ìš©í•œ ì´ìƒê°’ íƒì§€
    """
    mean = np.mean(data[column])
    std = np.std(data[column])
    threshold = 3 * std
    
    anomalies = data[
        (data[column] < mean - threshold) | 
        (data[column] > mean + threshold)
    ]
    
    return anomalies
```

#### ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê¸°ë°˜ íƒì§€
```python
def detect_business_rule_violations(ticket_data):
    """
    ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ìœ„ë°˜ íƒì§€
    """
    violations = []
    
    # í•´ê²° ì‹œê°„ì´ ìƒì„± ì‹œê°„ë³´ë‹¤ ì´ë¥¸ ê²½ìš°
    if ticket_data['resolved_at'] < ticket_data['created_at']:
        violations.append("Invalid resolution time")
    
    # ìš°ì„ ìˆœìœ„ì™€ í•´ê²° ì‹œê°„ ë¶ˆì¼ì¹˜
    if (ticket_data['priority'] == 'Critical' and 
        ticket_data['resolution_time'] > 4):  # 4ì‹œê°„
        violations.append("SLA violation for critical ticket")
    
    return violations
```

## ì„±ëŠ¥ ìµœì í™”

### 1. ì¸ë±ì‹± ì „ëµ

#### ì‹œê°„ ê¸°ë°˜ íŒŒí‹°ì…”ë‹
```sql
-- ì›”ë³„ íŒŒí‹°ì…”ë‹
CREATE TABLE tickets_partitioned (
    id INT,
    created_at DATETIME,
    -- ê¸°íƒ€ í•„ë“œë“¤
) PARTITION BY RANGE (YEAR(created_at) * 100 + MONTH(created_at)) (
    PARTITION p202301 VALUES LESS THAN (202302),
    PARTITION p202302 VALUES LESS THAN (202303),
    PARTITION p202303 VALUES LESS THAN (202304)
);
```

#### ì»¬ëŸ¼ìŠ¤í† ì–´ ì¸ë±ìŠ¤
```sql
-- ë¶„ì„ ì¿¼ë¦¬ ìµœì í™”ë¥¼ ìœ„í•œ ì»¬ëŸ¼ìŠ¤í† ì–´ ì¸ë±ìŠ¤
CREATE COLUMNSTORE INDEX IX_tickets_analytics
ON tickets (created_at, priority, category, status, assigned_to);
```

### 2. ì¿¼ë¦¬ ìµœì í™”

#### ì§‘ê³„ í…Œì´ë¸” ì‚¬ì „ ê³„ì‚°
```sql
-- ì‹œê°„ë³„ ì§‘ê³„ í…Œì´ë¸”
CREATE MATERIALIZED VIEW hourly_ticket_stats AS
SELECT 
    DATE_FORMAT(created_at, '%Y-%m-%d %H:00:00') as hour_bucket,
    COUNT(*) as ticket_count,
    COUNT(CASE WHEN priority = 'Critical' THEN 1 END) as critical_count,
    AVG(TIMESTAMPDIFF(MINUTE, created_at, first_response_at)) as avg_response_time
FROM tickets
GROUP BY DATE_FORMAT(created_at, '%Y-%m-%d %H:00:00');
```

#### ìºì‹± ì „ëµ
```python
class AnalyticsCache:
    def __init__(self):
        self.cache = {}
        self.cache_ttl = {}
    
    def get_cached_result(self, query_hash):
        """
        ìºì‹œëœ ì¿¼ë¦¬ ê²°ê³¼ ì¡°íšŒ
        """
        if query_hash in self.cache:
            if time.time() < self.cache_ttl[query_hash]:
                return self.cache[query_hash]
            else:
                # ë§Œë£Œëœ ìºì‹œ ì œê±°
                del self.cache[query_hash]
                del self.cache_ttl[query_hash]
        
        return None
    
    def cache_result(self, query_hash, result, ttl_seconds=3600):
        """
        ì¿¼ë¦¬ ê²°ê³¼ ìºì‹±
        """
        self.cache[query_hash] = result
        self.cache_ttl[query_hash] = time.time() + ttl_seconds
```

## ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬

### 1. ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì•„í‚¤í…ì²˜

#### Apache Kafka í™œìš©
```python
def setup_kafka_producer():
    """
    Kafka í”„ë¡œë“€ì„œ ì„¤ì •
    """
    producer = KafkaProducer(
        bootstrap_servers=['kafka1:9092', 'kafka2:9092'],
        value_serializer=lambda v: json.dumps(v).encode('utf-8'),
        compression_type='gzip'
    )
    return producer

def publish_ticket_event(producer, event_type, ticket_data):
    """
    í‹°ì¼“ ì´ë²¤íŠ¸ ë°œí–‰
    """
    event = {
        'event_type': event_type,
        'timestamp': datetime.utcnow().isoformat(),
        'data': ticket_data
    }
    
    producer.send('ticket-events', value=event)
```

### 2. ì‹¤ì‹œê°„ ì§‘ê³„ ì²˜ë¦¬

#### Apache Spark Streaming
```python
from pyspark.streaming import StreamingContext
from pyspark.sql import SparkSession

def process_real_time_analytics():
    """
    ì‹¤ì‹œê°„ ë¶„ì„ ì²˜ë¦¬
    """
    spark = SparkSession.builder.appName("FreshserviceAnalytics").getOrCreate()
    ssc = StreamingContext(spark.sparkContext, 10)  # 10ì´ˆ ë°°ì¹˜
    
    # Kafkaì—ì„œ ìŠ¤íŠ¸ë¦¼ ìƒì„±
    kafka_stream = KafkaUtils.createDirectStream(
        ssc, 
        ['ticket-events'], 
        {"metadata.broker.list": "kafka1:9092,kafka2:9092"}
    )
    
    # ì‹¤ì‹œê°„ ì§‘ê³„ ì²˜ë¦¬
    ticket_counts = kafka_stream.map(lambda x: json.loads(x[1])) \
                               .map(lambda event: (event['data']['priority'], 1)) \
                               .reduceByKey(lambda a, b: a + b)
    
    ticket_counts.pprint()
    
    ssc.start()
    ssc.awaitTermination()
```

## ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### 1. ETL í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§

#### ì²˜ë¦¬ ìƒíƒœ ì¶”ì 
```python
class ETLMonitor:
    def __init__(self):
        self.metrics = {
            'extract_count': 0,
            'transform_count': 0,
            'load_count': 0,
            'error_count': 0,
            'processing_time': 0
        }
    
    def log_stage_completion(self, stage, count, duration):
        """
        ETL ë‹¨ê³„ ì™„ë£Œ ë¡œê¹…
        """
        self.metrics[f'{stage}_count'] = count
        self.metrics['processing_time'] += duration
        
        # ì„±ëŠ¥ ì„ê³„ê°’ ì²´í¬
        if duration > self.get_threshold(stage):
            self.send_alert(f"{stage} stage took {duration}s")
```

### 2. ë°ì´í„° í’ˆì§ˆ ì•Œë¦¼

#### ìë™ ì•Œë¦¼ ì‹œìŠ¤í…œ
```python
def check_data_quality_and_alert():
    """
    ë°ì´í„° í’ˆì§ˆ ì²´í¬ ë° ì•Œë¦¼
    """
    # ë°ì´í„° ì™„ê²°ì„± ì²´í¬
    missing_data_pct = calculate_missing_data_percentage()
    if missing_data_pct > 5:  # 5% ì´ìƒ ëˆ„ë½
        send_alert(f"High missing data percentage: {missing_data_pct}%")
    
    # ë°ì´í„° ì§€ì—° ì²´í¬
    latest_data_time = get_latest_data_timestamp()
    current_time = datetime.utcnow()
    delay_minutes = (current_time - latest_data_time).total_seconds() / 60
    
    if delay_minutes > 30:  # 30ë¶„ ì´ìƒ ì§€ì—°
        send_alert(f"Data processing delayed by {delay_minutes} minutes")
```

ì´ëŸ¬í•œ ì²´ê³„ì ì¸ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì™€ ETL í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ Analytics í”Œë«í¼ì˜ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì„ ë³´ì¥í•˜ê³ , ì •í™•í•˜ê³  ì‹ ì†í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.