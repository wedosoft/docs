#!/usr/bin/env python3
"""
FAQ ë³µì¡ë„ ë¶„ì„ê¸°
ì´ë¯¸ì§€, í…Œì´ë¸”, ì„œì‹ ë“±ì„ í¬í•¨í•œ ì¤‘ìš” ë¬¸ì„œì™€ ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ êµ¬ë¶„
"""

import pandas as pd
import re
import json
from pathlib import Path
import html

def analyze_content_complexity(html_content):
    """HTML ì½˜í…ì¸ ì˜ ë³µì¡ë„ë¥¼ ë¶„ì„"""
    if pd.isna(html_content) or html_content == '':
        return {
            'complexity_score': 0,
            'has_images': False,
            'has_tables': False,
            'has_code_blocks': False,
            'has_lists': False,
            'has_formatting': False,
            'image_count': 0,
            'content_length': 0,
            'processing_recommendation': 'AUTO'
        }
    
    html_str = str(html_content)
    content_length = len(html_str)
    complexity_score = 0
    
    # ì´ë¯¸ì§€ ë¶„ì„
    image_count = html_str.count('<img')
    has_images = image_count > 0
    if has_images:
        complexity_score += 5 * image_count  # ì´ë¯¸ì§€ë§ˆë‹¤ 5ì 
    
    # í…Œì´ë¸” ë¶„ì„
    has_tables = '<table' in html_str or '<th>' in html_str or '<td>' in html_str
    if has_tables:
        table_count = html_str.count('<table')
        complexity_score += 3 * table_count  # í…Œì´ë¸”ë§ˆë‹¤ 3ì 
    
    # ì½”ë“œ ë¸”ë¡ ë¶„ì„
    has_code_blocks = '<pre>' in html_str or '<code>' in html_str
    if has_code_blocks:
        code_count = html_str.count('<pre>') + html_str.count('<code>')
        complexity_score += 2 * code_count  # ì½”ë“œ ë¸”ë¡ë§ˆë‹¤ 2ì 
    
    # ë¦¬ìŠ¤íŠ¸ ë¶„ì„
    has_lists = '<ul>' in html_str or '<ol>' in html_str
    if has_lists:
        list_count = html_str.count('<ul>') + html_str.count('<ol>')
        complexity_score += 1 * list_count  # ë¦¬ìŠ¤íŠ¸ë§ˆë‹¤ 1ì 
    
    # ì„œì‹ ë¶„ì„ (êµµê²Œ, ê¸°ìš¸ì„, ì œëª© ë“±)
    formatting_patterns = ['<strong>', '<b>', '<em>', '<i>', '<h1>', '<h2>', '<h3>', '<h4>', '<h5>', '<h6>']
    has_formatting = any(pattern in html_str for pattern in formatting_patterns)
    if has_formatting:
        formatting_count = sum(html_str.count(pattern) for pattern in formatting_patterns)
        complexity_score += 0.5 * formatting_count  # ì„œì‹ë§ˆë‹¤ 0.5ì 
    
    # ì „ì²´ ê¸¸ì´ ë³´ë„ˆìŠ¤
    if content_length > 2000:
        complexity_score += 2
    elif content_length > 1000:
        complexity_score += 1
    
    # ì²˜ë¦¬ ë°©ë²• ê²°ì •
    if complexity_score >= 10:
        processing_recommendation = 'MANUAL'  # ìˆ˜ë™ ì²˜ë¦¬
    elif complexity_score >= 5:
        processing_recommendation = 'REVIEW'  # ê²€í†  í•„ìš”
    else:
        processing_recommendation = 'AUTO'    # ìë™ ì²˜ë¦¬
    
    return {
        'complexity_score': complexity_score,
        'has_images': has_images,
        'has_tables': has_tables,
        'has_code_blocks': has_code_blocks,
        'has_lists': has_lists,
        'has_formatting': has_formatting,
        'image_count': image_count,
        'content_length': content_length,
        'processing_recommendation': processing_recommendation
    }

def extract_categories_from_path(path):
    """ê²½ë¡œì—ì„œ ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ì¶œ"""
    if pd.isna(path):
        return 'unknown'
    
    path_str = str(path).lower()
    
    # ì£¼ìš” ì¹´í…Œê³ ë¦¬ íŒ¨í„´ ë§¤ì¹­
    if 'automation' in path_str:
        return 'automations'
    elif 'change' in path_str:
        return 'changes'
    elif 'asset' in path_str:
        return 'asset-management'
    elif 'report' in path_str:
        return 'reports'
    elif 'ticket' in path_str:
        return 'tickets'
    elif 'service' in path_str:
        return 'service-catalog'
    elif 'project' in path_str:
        return 'projects'
    elif 'release' in path_str:
        return 'releases'
    elif 'problem' in path_str:
        return 'problems'
    elif 'user' in path_str or 'requester' in path_str:
        return 'users'
    else:
        return 'misc'

def analyze_all_faqs():
    """ëª¨ë“  FAQ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ë³µì¡ë„ë³„ë¡œ ë¶„ë¥˜"""
    
    # CSV íŒŒì¼ë“¤ ì½ê¸°
    csv_files = [
        'raw_data/merged_articles_links_replaced_freshservice_part1.csv',
        'raw_data/merged_articles_links_replaced_freshservice_part2.csv', 
        'raw_data/merged_articles_links_replaced_freshservice_part3.csv',
        'raw_data/merged_articles_links_replaced_freshservice_part4.csv',
        'raw_data/merged_articles_links_replaced_freshservice_part5.csv'
    ]
    
    all_results = []
    
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file)
            print(f"ì²˜ë¦¬ ì¤‘: {csv_file} ({len(df)}ê°œ í•­ëª©)")
            
            for idx, row in df.iterrows():
                title = str(row.get('title', ''))
                description = str(row.get('description', ''))
                html_content = str(row.get('desc_un_html', ''))
                path = str(row.get('path', ''))
                
                # ë³µì¡ë„ ë¶„ì„
                complexity = analyze_content_complexity(html_content)
                category = extract_categories_from_path(path)
                
                result = {
                    'title': title,
                    'category': category,
                    'path': path,
                    'description_preview': description[:100] + '...' if len(description) > 100 else description,
                    **complexity
                }
                
                all_results.append(result)
                
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ {csv_file}: {e}")
    
    return all_results

def generate_summary_report(results):
    """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
    by_category = {}
    for result in results:
        category = result['category']
        if category not in by_category:
            by_category[category] = []
        by_category[category].append(result)
    
    # ì²˜ë¦¬ ë°©ë²•ë³„ ë¶„ë¥˜
    manual_count = len([r for r in results if r['processing_recommendation'] == 'MANUAL'])
    review_count = len([r for r in results if r['processing_recommendation'] == 'REVIEW'])
    auto_count = len([r for r in results if r['processing_recommendation'] == 'AUTO'])
    
    print("=" * 60)
    print("ğŸ“Š FAQ ë³µì¡ë„ ë¶„ì„ ê²°ê³¼")
    print("=" * 60)
    print(f"ì´ FAQ ìˆ˜: {len(results)}")
    print(f"ğŸ”¥ ìˆ˜ë™ ì²˜ë¦¬ í•„ìš”: {manual_count}ê°œ ({manual_count/len(results)*100:.1f}%)")
    print(f"âš¡ ê²€í†  í•„ìš”: {review_count}ê°œ ({review_count/len(results)*100:.1f}%)")
    print(f"ğŸ¤– ìë™ ì²˜ë¦¬ ê°€ëŠ¥: {auto_count}ê°œ ({auto_count/len(results)*100:.1f}%)")
    print()
    
    # ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ë¶„ì„
    print("ğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„:")
    print("-" * 40)
    
    for category, items in sorted(by_category.items(), key=lambda x: len(x[1]), reverse=True):
        manual = len([i for i in items if i['processing_recommendation'] == 'MANUAL'])
        review = len([i for i in items if i['processing_recommendation'] == 'REVIEW'])
        auto = len([i for i in items if i['processing_recommendation'] == 'AUTO'])
        
        print(f"{category}: {len(items)}ê°œ")
        print(f"  - ìˆ˜ë™: {manual}ê°œ, ê²€í† : {review}ê°œ, ìë™: {auto}ê°œ")
    
    print()
    
    # ìˆ˜ë™ ì²˜ë¦¬ê°€ í•„ìš”í•œ í•­ëª©ë“¤ ìƒì„¸ ì •ë³´
    manual_items = [r for r in results if r['processing_recommendation'] == 'MANUAL']
    if manual_items:
        print("ğŸ”¥ ìˆ˜ë™ ì²˜ë¦¬ í•„ìš” í•­ëª©ë“¤ (ìƒìœ„ 10ê°œ):")
        print("-" * 40)
        
        for item in sorted(manual_items, key=lambda x: x['complexity_score'], reverse=True)[:10]:
            features = []
            if item['has_images']:
                features.append(f"ì´ë¯¸ì§€ {item['image_count']}ê°œ")
            if item['has_tables']:
                features.append("í…Œì´ë¸”")
            if item['has_code_blocks']:
                features.append("ì½”ë“œ")
            if item['has_lists']:
                features.append("ëª©ë¡")
            
            print(f"- [{item['category']}] {item['title'][:50]}...")
            print(f"  ì ìˆ˜: {item['complexity_score']:.1f}, íŠ¹ì§•: {', '.join(features)}")
        print()
    
    return {
        'total': len(results),
        'manual': manual_count,
        'review': review_count,
        'auto': auto_count,
        'by_category': by_category
    }

def save_detailed_results(results, output_file='faq_complexity_analysis.json'):
    """ìƒì„¸ ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    output_path = Path(output_file)
    
    # JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ float ê°’ ì²˜ë¦¬
    for result in results:
        result['complexity_score'] = float(result['complexity_score'])
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_path}")

if __name__ == "__main__":
    print("ğŸ” FAQ ë³µì¡ë„ ë¶„ì„ ì‹œì‘...")
    
    # ì „ì²´ ë¶„ì„ ì‹¤í–‰
    results = analyze_all_faqs()
    
    # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
    summary = generate_summary_report(results)
    
    # ìƒì„¸ ê²°ê³¼ ì €ì¥
    save_detailed_results(results)
    
    print("\nâœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“Š ìš”ì•½: ìˆ˜ë™ {summary['manual']}ê°œ, ê²€í†  {summary['review']}ê°œ, ìë™ {summary['auto']}ê°œ")
